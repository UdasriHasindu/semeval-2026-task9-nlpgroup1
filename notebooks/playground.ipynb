{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9f8c34",
   "metadata": {},
   "source": [
    "## Subtask 01: Binary classification\n",
    "\n",
    "`polarized` or `non-polarized`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e805322f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-50af8a16-2f25-424f-a634-0188c3a24a15\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>polarization</th>\n",
       "      <th>political</th>\n",
       "      <th>racial/ethnic</th>\n",
       "      <th>religious</th>\n",
       "      <th>gender/sexual</th>\n",
       "      <th>other</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>vilification</th>\n",
       "      <th>dehumanization</th>\n",
       "      <th>extreme_language</th>\n",
       "      <th>lack_of_empathy</th>\n",
       "      <th>invalidation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>እንደምን አደራችሁ ቤዛ! የ እለቱን ቃል በ ፓስተር ፍቅሬ በላይ እነሆ! ...</td>\n",
       "      <td>amh</td>\n",
       "      <td>amh_4a249cbbca6389d5120fe9b556f5ebf2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER ተሌግራምን እንደ አማራጭ መጠቀም መልካም ይመስለኛል::</td>\n",
       "      <td>amh</td>\n",
       "      <td>amh_01b5780721c676222b91ec97a797a066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>አንደኛው አመት የፕሪቶርያው ስምምነት ምክንያት በማድረግ ከህዝባዊ ወያነ ...</td>\n",
       "      <td>amh</td>\n",
       "      <td>amh_7aabf3f2c41cbd032e6dbf56eccca9d7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ግደል እንዳልልህ.ሃጢያት ነው አውቃለሁ አትግደል አልልህ.ጨካኝ ፍጡር ናቸ...</td>\n",
       "      <td>amh</td>\n",
       "      <td>amh_16df45b84088cd0d09ca4fa3a0e5f35c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>አለ ነገር አለ ነገር እንደው አለው ነገር እንዲህ ታመን ታመን አንገት ደ...</td>\n",
       "      <td>amh</td>\n",
       "      <td>amh_a28c14c4242c9387bc7a028cfad19c9f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50af8a16-2f25-424f-a634-0188c3a24a15')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-50af8a16-2f25-424f-a634-0188c3a24a15 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-50af8a16-2f25-424f-a634-0188c3a24a15');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                text lang  \\\n",
       "0  እንደምን አደራችሁ ቤዛ! የ እለቱን ቃል በ ፓስተር ፍቅሬ በላይ እነሆ! ...  amh   \n",
       "1           @USER ተሌግራምን እንደ አማራጭ መጠቀም መልካም ይመስለኛል::  amh   \n",
       "2  አንደኛው አመት የፕሪቶርያው ስምምነት ምክንያት በማድረግ ከህዝባዊ ወያነ ...  amh   \n",
       "3  ግደል እንዳልልህ.ሃጢያት ነው አውቃለሁ አትግደል አልልህ.ጨካኝ ፍጡር ናቸ...  amh   \n",
       "4  አለ ነገር አለ ነገር እንደው አለው ነገር እንዲህ ታመን ታመን አንገት ደ...  amh   \n",
       "\n",
       "                                     id  polarization  political  \\\n",
       "0  amh_4a249cbbca6389d5120fe9b556f5ebf2             0          0   \n",
       "1  amh_01b5780721c676222b91ec97a797a066             0          0   \n",
       "2  amh_7aabf3f2c41cbd032e6dbf56eccca9d7             0          0   \n",
       "3  amh_16df45b84088cd0d09ca4fa3a0e5f35c             0          0   \n",
       "4  amh_a28c14c4242c9387bc7a028cfad19c9f             0          0   \n",
       "\n",
       "   racial/ethnic  religious  gender/sexual  other  stereotype  vilification  \\\n",
       "0              0          0              0      0           0             0   \n",
       "1              0          0              0      0           0             0   \n",
       "2              0          0              0      0           0             0   \n",
       "3              0          0              0      0           0             0   \n",
       "4              0          0              0      0           0             0   \n",
       "\n",
       "   dehumanization  extreme_language  lack_of_empathy  invalidation  \n",
       "0               0                 0                0             0  \n",
       "1               0                 0                0             0  \n",
       "2               0                 0                0             0  \n",
       "3               0                 0                0             0  \n",
       "4               0                 0                0             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Polar-SemEval/trial-data/refs/heads/main/Trial_Data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae9cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 338 entries, 0 to 337\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   text              338 non-null    object\n",
      " 1   lang              338 non-null    object\n",
      " 2   id                338 non-null    object\n",
      " 3   polarization      338 non-null    int64 \n",
      " 4   political         338 non-null    int64 \n",
      " 5   racial/ethnic     338 non-null    int64 \n",
      " 6   religious         338 non-null    int64 \n",
      " 7   gender/sexual     338 non-null    int64 \n",
      " 8   other             338 non-null    int64 \n",
      " 9   stereotype        338 non-null    int64 \n",
      " 10  vilification      338 non-null    int64 \n",
      " 11  dehumanization    338 non-null    int64 \n",
      " 12  extreme_language  338 non-null    int64 \n",
      " 13  lack_of_empathy   338 non-null    int64 \n",
      " 14  invalidation      338 non-null    int64 \n",
      "dtypes: int64(12), object(3)\n",
      "memory usage: 39.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0127f",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ae5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b54fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df['text']\n",
    "y = df['polarization']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273ce8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270,) (68,) (270,) (68,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0398b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e20000",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a3025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9377b4b3f04ec2ae532704d979957f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8998f065b91d4729afbc4561c9827c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab8f1afe45348d3b48f88fa8a3ba16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eed7b758034407d9f5486894bf9f007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4417e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([16, 138]), 'attention_mask': torch.Size([16, 138]), 'labels': torch.Size([16, 1])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Dataset\n",
    "class PolarizationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        \"\"\"\n",
    "        texts: pd.Series or list of strings\n",
    "        labels: pd.Series or list of ints (0/1)\n",
    "        tokenizer: transformers tokenizer\n",
    "        \"\"\"\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = int(self.labels.iloc[idx])   # ensure 0/1 ints\n",
    "\n",
    "        # Tokenize single example (we'll batch-pad in DataLoader using collate_fn)\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False  # XLM-R doesn't use token_type_ids\n",
    "        )\n",
    "\n",
    "        # Convert to tensors here; collate_fn will stack them\n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(encoded[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(encoded[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)  # float for BCEWithLogitsLoss\n",
    "        }\n",
    "        return item\n",
    "\n",
    "# Collate function to pad a batch (recommended)\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of items from __getitem__\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    attention_mask = [b[\"attention_mask\"] for b in batch]\n",
    "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
    "\n",
    "    # pad with tokenizer (convenient)\n",
    "    padded = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    padded[\"labels\"] = labels.unsqueeze(1)  # shape (batch, 1) for BCEWithLogits\n",
    "    return padded\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = PolarizationDataset(x_train, y_train, tokenizer, max_length=256)\n",
    "test_dataset  = PolarizationDataset(x_test, y_test, tokenizer, max_length=256)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,       # adjust for your machine; 0 on Windows sometimes safer\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Quick smoke test: fetch one batch\n",
    "batch = next(iter(train_loader))\n",
    "print({k: v.shape for k, v in batch.items()})\n",
    "# expected keys: input_ids, attention_mask, labels\n",
    "# e.g. {'input_ids': (BATCH_SIZE, seq_len), 'attention_mask': (BATCH_SIZE, seq_len), 'labels': (BATCH_SIZE, 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6c09df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace5fb21c3e841f78a43953f4db91f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "# Binary classification → 2 labels: polarized vs not polarized\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c0a776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
